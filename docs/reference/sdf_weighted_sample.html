---
title: "Perform Weighted Random Sampling on a Spark DataFrame"
aliases:
  - reference/sparklyr/latest/sdf_weighted_sample.html
---

    <div>

    <div>
    <ul data-gumshoe>
    <li><a href="#arguments">Arguments</a></li>
    
    <li><a href="#transforming-spark-dataframes">Transforming Spark DataFrames</a></li>

    <li><a href="#see-also">See also</a></li>
        </ul>
    </div>

    <div>

    
    <p>Draw a random sample of rows (with or without replacement) from a Spark
DataFrame
If the sampling is done without replacement, then it will be conceptually
equivalent to an iterative process such that in each step the probability of
adding a row to the sample set is equal to its weight divided by summation of
weights of all rows that are not in the sample set yet in that step.</p>
    

    <div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class='fu'>sdf_weighted_sample</span>(<span class='no'>x</span>, <span class='no'>weight_col</span>, <span class='no'>k</span>, <span class='kw'>replacement</span> <span class='kw'>=</span> <span class='fl'>TRUE</span>, <span class='kw'>seed</span> <span class='kw'>=</span> <span class='kw'>NULL</span>)</code></pre></div>
    
    <h2 id="arguments">Arguments</h2>
    <table class="ref-arguments">

    <colgroup>
      <col class="name" />
      <col class="desc" />
    </colgroup>

    <tr>
      <td>x</td>
      <td><p>An object coercable to a Spark DataFrame.</p></td>
    </tr>
    <tr>
      <td>weight_col</td>
      <td><p>Name of the weight column</p></td>
    </tr>
    <tr>
      <td>k</td>
      <td><p>Sample set size</p></td>
    </tr>
    <tr>
      <td>replacement</td>
      <td><p>Whether to sample with replacement</p></td>
    </tr>
    <tr>
      <td>seed</td>
      <td><p>An (optional) integer seed</p></td>
    </tr>
    </table>
    
    <h2 id="transforming-spark-dataframes">Transforming Spark DataFrames</h2>

    


<p>The family of functions prefixed with <code>sdf_</code> generally access the Scala
Spark DataFrame API directly, as opposed to the <code>dplyr</code> interface which
uses Spark SQL. These functions will 'force' any pending SQL in a
<code>dplyr</code> pipeline, such that the resulting <code>tbl_spark</code> object
returned will no longer have the attached 'lazy' SQL operations. Note that
the underlying Spark DataFrame <em>does</em> execute its operations lazily, so
that even though the pending set of operations (currently) are not exposed at
the <span style="R">R</span> level, these operations will only be executed when you explicitly
<code><a href='collect.html'>collect()</a></code> the table.</p>
    
    <h2 id="see-also">See also</h2>

    <div class='dont-index'><p>Other Spark data frames: 
<code><a href='sdf_copy_to.html'>sdf_copy_to</a>()</code>,
<code><a href='sdf_random_split.html'>sdf_random_split</a>()</code>,
<code><a href='sdf_register.html'>sdf_register</a>()</code>,
<code><a href='sdf_sample.html'>sdf_sample</a>()</code>,
<code><a href='sdf_sort.html'>sdf_sort</a>()</code></p></div>
    

    </div>

    </div>




