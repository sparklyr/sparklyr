---
title: "Apply an R Function in Spark"
aliases:
  - reference/sparklyr/latest/spark_apply.html
---

    <div>

    <div>
    <ul data-gumshoe>
    <li><a href="#arguments">Arguments</a></li>
    <li><a href="#configuration">Configuration</a></li>
    <li><a href="#examples">Examples</a></li>
    </ul>
    </div>

    <div>

    <p>Applies an R function to a Spark object (typically, a Spark DataFrame).</p>

    <div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class='fu'>spark_apply</span>(
  <span class='no'>x</span>,
  <span class='no'>f</span>,
  <span class='kw'>columns</span> <span class='kw'>=</span> <span class='kw'>NULL</span>,
  <span class='kw'>memory</span> <span class='kw'>=</span> !<span class='fu'><a href='https://rdrr.io/r/base/NULL.html'>is.null</a></span>(<span class='no'>name</span>),
  <span class='kw'>group_by</span> <span class='kw'>=</span> <span class='kw'>NULL</span>,
  <span class='kw'>packages</span> <span class='kw'>=</span> <span class='kw'>NULL</span>,
  <span class='kw'>context</span> <span class='kw'>=</span> <span class='kw'>NULL</span>,
  <span class='kw'>name</span> <span class='kw'>=</span> <span class='kw'>NULL</span>,
  <span class='kw'>barrier</span> <span class='kw'>=</span> <span class='kw'>NULL</span>,
  <span class='no'>...</span>
)</code></pre></div>

    <h2 id="arguments">Arguments</h2>
    <table class="ref-arguments">

    <colgroup>
      <col class="name" />
      <col class="desc" />
    </colgroup>

    <tr>
      <td>x</td>
      <td><p>An object (usually a <code>spark_tbl</code>) coercable to a Spark DataFrame.</p></td>
    </tr>
    <tr>
      <td>f</td>
      <td><p>A function that transforms a data frame partition into a data frame.
  The function <code>f</code> has signature <code>f(df, context, group1, group2, ...)</code> where
  <code>df</code> is a data frame with the data to be processed, <code>context</code>
  is an optional object passed as the <code>context</code> parameter and <code>group1</code> to
  <code>groupN</code> contain the values of the <code>group_by</code> values. When
  <code>group_by</code> is not specified, <code>f</code> takes only one argument.</p>
<p>Can also be an <code>rlang</code> anonymous function. For example, as <code>~ .x + 1</code>
  to define an expression that adds one to the given <code>.x</code> data frame.</p></td>
    </tr>
    <tr>
      <td>columns</td>
      <td><p>A vector of column names or a named vector of column types for
the transformed object. When not specified, a sample of 10 rows is taken to
infer out the output columns automatically, to avoid this performance penalty,
specify the column types. The sample size is configurable using the
<code>sparklyr.apply.schema.infer</code> configuration option.</p></td>
    </tr>
    <tr>
      <td>memory</td>
      <td><p>Boolean; should the table be cached into memory?</p></td>
    </tr>
    <tr>
      <td>group_by</td>
      <td><p>Column name used to group by data frame partitions.</p></td>
    </tr>
    <tr>
      <td>packages</td>
      <td><p>Boolean to distribute <code><a href='https://rdrr.io/r/base/libPaths.html'>.libPaths()</a></code> packages to each node,
  a list of packages to distribute, or a package bundle created with
  <code><a href='spark_apply_bundle.html'>spark_apply_bundle()</a></code>.</p>
<p>Defaults to <code>TRUE</code> or the <code>sparklyr.apply.packages</code> value set in
  <code><a href='spark_config.html'>spark_config()</a></code>.</p>
<p>For clusters using Yarn cluster mode, <code>packages</code> can point to a package
  bundle created using <code><a href='spark_apply_bundle.html'>spark_apply_bundle()</a></code> and made available as a Spark
  file using <code>config$sparklyr.shell.files</code>. For clusters using Livy, packages
  can be manually installed on the driver node.</p>
<p>For offline clusters where <code><a href='https://rdrr.io/r/utils/available.packages.html'>available.packages()</a></code> is not available,
  manually download the packages database from
 https://cran.r-project.org/web/packages/packages.rds and set
  <code><a href='https://rdrr.io/r/base/Sys.setenv.html'>Sys.setenv(sparklyr.apply.packagesdb = "&lt;pathl-to-rds&gt;")</a></code>. Otherwise,
  all packages will be used by default.</p>
<p>For clusters where R packages already installed in every worker node,
  the <code>spark.r.libpaths</code> config entry can be set in <code><a href='spark_config.html'>spark_config()</a></code>
  to the local packages library. To specify multiple paths collapse them
  (without spaces) with a comma delimiter (e.g., <code>"/lib/path/one,/lib/path/two"</code>).</p></td>
    </tr>
    <tr>
      <td>context</td>
      <td><p>Optional object to be serialized and passed back to <code>f()</code>.</p></td>
    </tr>
    <tr>
      <td>name</td>
      <td><p>Optional table name while registering the resulting data frame.</p></td>
    </tr>
    <tr>
      <td>barrier</td>
      <td><p>Optional to support Barrier Execution Mode in the scheduler.</p></td>
    </tr>
    <tr>
      <td>...</td>
      <td><p>Optional arguments; currently unused.</p></td>
    </tr>
    </table>

    <h2 id="configuration">Configuration</h2>

    


<p><code><a href='spark_config.html'>spark_config()</a></code> settings can be specified to change the workers
environment.</p>
<p>For instance, to set additional environment variables to each
worker node use the <code>sparklyr.apply.env.*</code> config, to launch workers
without <code>--vanilla</code> use <code>sparklyr.apply.options.vanilla</code> set to
<code>FALSE</code>, to run a custom script before launching Rscript use
<code>sparklyr.apply.options.rscript.before</code>.</p>

    <h2 id="examples">Examples</h2>
    <div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><div class='input'><span class='kw'>if</span> (<span class='fl'>FALSE</span>) {

<span class='fu'><a href='https://rdrr.io/r/base/library.html'>library</a></span>(<span class='no'>sparklyr</span>)
<span class='no'>sc</span> <span class='kw'>&lt;-</span> <span class='fu'><a href='spark-connections.html'>spark_connect</a></span>(<span class='kw'>master</span> <span class='kw'>=</span> <span class='st'>"local[3]"</span>)

<span class='co'># creates an Spark data frame with 10 elements then multiply times 10 in R</span>
<span class='fu'><a href='sdf_len.html'>sdf_len</a></span>(<span class='no'>sc</span>, <span class='fl'>10</span>) <span class='kw'>%&gt;%</span> <span class='fu'>spark_apply</span>(<span class='kw'>function</span>(<span class='no'>df</span>) <span class='no'>df</span> * <span class='fl'>10</span>)

<span class='co'># using barrier mode</span>
<span class='fu'><a href='sdf_len.html'>sdf_len</a></span>(<span class='no'>sc</span>, <span class='fl'>3</span>, <span class='kw'>repartition</span> <span class='kw'>=</span> <span class='fl'>3</span>) <span class='kw'>%&gt;%</span>
  <span class='fu'>spark_apply</span>(<span class='no'>nrow</span>, <span class='kw'>barrier</span> <span class='kw'>=</span> <span class='fl'>TRUE</span>, <span class='kw'>columns</span> <span class='kw'>=</span> <span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span>(<span class='kw'>id</span> <span class='kw'>=</span> <span class='st'>"integer"</span>)) <span class='kw'>%&gt;%</span>
  <span class='fu'><a href='collect.html'>collect</a></span>()
}</div></code></pre></div>

    </div>

    </div>




