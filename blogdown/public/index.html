<!DOCTYPE html>
<html>
<head>
  <script>
    theBaseUrl = location.origin + "/";
  </script>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width,user-scalable=no,initial-scale=1,maximum-scale=1">
  <title>sparklyr</title>
  <meta name="generator" content="Hugo 0.26" />

  
  <meta name="description" content="An R interface to Spark">
  
  <link rel="canonical" href="/">
  

  <meta property="og:url" content="/">
  <meta property="og:title" content="sparklyr">
  <meta name="apple-mobile-web-app-title" content="sparklyr">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

  <style>
    @font-face {
      font-family: 'Icon';
      src: url('fonts/icon.eot?52m981');
      src: url('fonts/icon.eot?#iefix52m981')
      format('embedded-opentype'),
      url('fonts/icon.woff?52m981')
      format('woff'),
      url('fonts/icon.ttf?52m981')
      format('truetype'),
      url('fonts/icon.svg?52m981#icon')
      format('svg');
      font-weight: normal;
      font-style: normal;
    }
  </style>


  <link rel="stylesheet" href="/css/style.css">
  <link rel="stylesheet" href="/css/site-styles.css">

  <script src="/js/vendor.js"></script>

  <script src="/js/app.js"></script>
  
  <link rel="shortcut icon" href="/images/favicon.png">

  
  <link href="/blog/index.xml" rel="alternate" type="application/rss+xml" title="sparklyr" />
  <link href="/blog/index.xml" rel="feed" type="application/rss+xml" title="sparklyr" />
  
</head>
<body>


<div class="single-page">
  <nav data-gumshoe-header aria-label="Header" id="header" class="top-menu">
  <div class="left">
     <a href = "/" data-parenturl="" class="top-menu-item site-title">sparklyr</a>
     <span class="logo-from">from</span>
     <a href="/">
      <div id="logo" class="logo"></div>
    </a>
  </div>


  <div class="right">
    <div class="top-menu-items" id="top-menu-items">
        <a href="/dplyr" data-parenturl="/dplyr" class="top-menu-item">dplyr</a>
        <a href="/mlib" data-parenturl="/mlib" class="top-menu-item">MLib</a>
        <a href="/extensions" data-parenturl="/extensions" class="top-menu-item">Extensions</a>
        <a href="/news" data-parenturl="/news" class="top-menu-item">News</a>
        <a href="/reference" data-parenturl="/reference" class="top-menu-item">Reference</a>
    </div>
    

    
    
  </div>
</nav>

<nav aria-label="Header" id="mobile-header">
  <a href = "/">sparklyr</a>
  <div class="right">
    <a href="https://github.com/rstudio/sparklyr" class="github-logo">
      <i class="fa fa-lg fa-github" aria-hidden="true"></i>
    </a>
    <div class="hamburger">
      <i class="fa fa-lg fa-bars" aria-hidden="true"></i>
    </div>

  </div>

</nav>

<div id="mobile-menu-container">
<ul id="mobile-menu">
  
    <li>
      <a href="">Using sparklyr</a>
       <i class="fa fa-chevron-right" aria-hidden="true"></i>
    </li>
    <ul>
    
      <li>
        <a href="/guides/connections/">Getting Started</a>
        
      </li>
      
    
      <li>
        <a href="/dplyr/">Manipulating data</a>
        
      </li>
      
    
      <li>
        <a href="/mlib/">Machine Learning</a>
        
      </li>
      
    
      <li>
        <a href="/guides/caching/">Understanding Caching</a>
        
      </li>
      
    
      <li>
        <a href="/deployment/">Deployment Options</a>
        
      </li>
      
    
    </ul>
  
    <li>
      <a href="">Guides</a>
       <i class="fa fa-chevron-right" aria-hidden="true"></i>
    </li>
    <ul>
    
      <li>
        <a href="/guides/distributed-r/">Distributed R</a>
        
      </li>
      
    
      <li>
        <a href="/guides/data-lakes/">Data Lakes</a>
        
      </li>
      
    
      <li>
        <a href="/extensions/">Extend sparklyr</a>
        
      </li>
      
    
      <li>
        <a href="/guides/textmining">Text mining</a>
        
      </li>
      
    
    </ul>
  
    <li>
      <a href="/examples/">Deployment Examples</a>
       <i class="fa fa-chevron-right" aria-hidden="true"></i>
    </li>
    <ul>
    
      <li>
        <a href="/examples/stand-alone-aws/">Standalone cluster (AWS)</a>
        
      </li>
      
    
      <li>
        <a href="/examples/yarn-cluster-emr/">YARN cluster (EMR)</a>
        
      </li>
      
    
      <li>
        <a href="/examples/cloudera-aws/">Cloudera cluster</a>
        
      </li>
      
    
    </ul>
  
    <li>
      <a href="/reference/">Reference</a>
       
    </li>
    <ul>
    
    </ul>
  
</ul>
</div>

<div id="search-bar" class="search-bar">
  <p class="search-bar__icon"><i class="fa fa-lg fa-search"></i> </p>
  <div class="search-bar__input">
      <input type="text" name="search" class="st-default-search-input">
  </div>
  <div class="search-bar__exit">
    <i class="fa fa-lg fa-times"></i>
  </div>

  <div class="inline-search-results">
    <ul>

    </ul>
  </div>
</div>



</div>

<div class="page documentation">
    <div class="side-menu" id="side-menu">
    
    
  

    

        <a href="" class="side-menu__item__link">
          <img class="side-menu__item__link__icon" src="/images/icons/block-retina.png" alt="">
          <p class="side-menu__item__link__text">Using sparklyr</p>
        </a>

        <ul class="side-menu__sub">
          
            <li class="side-menu__sub__item ">
              <a id="menu-link-/guides/connections/" class="side-menu__sub__item__text" href="/guides/connections/" data-url="/guides/connections/">Getting Started</a>
            </li>
          
            <li class="side-menu__sub__item ">
              <a id="menu-link-/dplyr/" class="side-menu__sub__item__text" href="/dplyr/" data-url="/dplyr/">Manipulating data</a>
            </li>
          
            <li class="side-menu__sub__item ">
              <a id="menu-link-/mlib/" class="side-menu__sub__item__text" href="/mlib/" data-url="/mlib/">Machine Learning</a>
            </li>
          
            <li class="side-menu__sub__item ">
              <a id="menu-link-/guides/caching/" class="side-menu__sub__item__text" href="/guides/caching/" data-url="/guides/caching/">Understanding Caching</a>
            </li>
          
            <li class="side-menu__sub__item ">
              <a id="menu-link-/deployment/" class="side-menu__sub__item__text" href="/deployment/" data-url="/deployment/">Deployment Options</a>
            </li>
          
        </ul>
          

    

        <a href="" class="side-menu__item__link">
          <img class="side-menu__item__link__icon" src="/images/icons/block-retina.png" alt="">
          <p class="side-menu__item__link__text">Guides</p>
        </a>

        <ul class="side-menu__sub">
          
            <li class="side-menu__sub__item ">
              <a id="menu-link-/guides/distributed-r/" class="side-menu__sub__item__text" href="/guides/distributed-r/" data-url="/guides/distributed-r/">Distributed R</a>
            </li>
          
            <li class="side-menu__sub__item ">
              <a id="menu-link-/guides/data-lakes/" class="side-menu__sub__item__text" href="/guides/data-lakes/" data-url="/guides/data-lakes/">Data Lakes</a>
            </li>
          
            <li class="side-menu__sub__item ">
              <a id="menu-link-/extensions/" class="side-menu__sub__item__text" href="/extensions/" data-url="/extensions/">Extend sparklyr</a>
            </li>
          
            <li class="side-menu__sub__item ">
              <a id="menu-link-/guides/textmining" class="side-menu__sub__item__text" href="/guides/textmining" data-url="/guides/textmining">Text mining</a>
            </li>
          
        </ul>
          

    

        <a href="/examples/" class="side-menu__item__link">
          <img class="side-menu__item__link__icon" src="/images/icons/block-retina.png" alt="">
          <p class="side-menu__item__link__text">Deployment Examples</p>
        </a>

        <ul class="side-menu__sub">
          
            <li class="side-menu__sub__item ">
              <a id="menu-link-/examples/stand-alone-aws/" class="side-menu__sub__item__text" href="/examples/stand-alone-aws/" data-url="/examples/stand-alone-aws/">Standalone cluster (AWS)</a>
            </li>
          
            <li class="side-menu__sub__item ">
              <a id="menu-link-/examples/yarn-cluster-emr/" class="side-menu__sub__item__text" href="/examples/yarn-cluster-emr/" data-url="/examples/yarn-cluster-emr/">YARN cluster (EMR)</a>
            </li>
          
            <li class="side-menu__sub__item ">
              <a id="menu-link-/examples/cloudera-aws/" class="side-menu__sub__item__text" href="/examples/cloudera-aws/" data-url="/examples/cloudera-aws/">Cloudera cluster</a>
            </li>
          
        </ul>
          

    

        <a href="/reference/" class="side-menu__item__link">
          <img class="side-menu__item__link__icon" src="/images/icons/block-retina.png" alt="">
          <p class="side-menu__item__link__text">Reference</p>
        </a>

        <ul class="side-menu__sub">
          
        </ul>
          

    
    
    
    </div>
    <div class="content">

      <h1 class="content-header"> </h1>

      <div class="markdowned">
      <div class="doc-page">
    
      <div class="doc-page-index">
      <ul id="gumshoe-container" data-gumshoe>
      </ul>
      </div>
  
      <div class="doc-page-body">  
        

<h1 id="sparklyr-r-interface-for-apache-spark">sparklyr: R interface for Apache Spark</h1>

<p><a href="https://travis-ci.org/rstudio/sparklyr"><img src="https://travis-ci.org/rstudio/sparklyr.svg?branch=master" alt="Build Status" /></a> <a href="https://cran.r-project.org/package=sparklyr"><img src="https://www.r-pkg.org/badges/version/sparklyr" alt="CRAN\_Status\_Badge" /></a> <a href="https://codecov.io/gh/rstudio/sparklyr"><img src="https://codecov.io/gh/rstudio/sparklyr/branch/master/graph/badge.svg" alt="codecov" /></a> <a href="https://gitter.im/rstudio/sparklyr?utm_source=badge&amp;utm_medium=badge&amp;utm_campaign=pr-badge&amp;utm_content=badge"><img src="https://badges.gitter.im/rstudio/sparklyr.svg" alt="Join the chat at https://gitter.im/rstudio/sparklyr" /></a></p>

<p><img src="tools/readme/sparklyr-illustration.png" width=364 height=197 align="right"/></p>

<ul>
<li>Connect to <a href="http://spark.apache.org/">Spark</a> from R. The sparklyr package provides a <br/> complete <a href="https://github.com/hadley/dplyr">dplyr</a> backend.</li>
<li>Filter and aggregate Spark datasets then bring them into R for <br/> analysis and visualization.</li>
<li>Use Spark&rsquo;s distributed <a href="http://spark.apache.org/docs/latest/mllib-guide.html">machine learning</a> library from R.</li>
<li>Create <a href="http://spark.rstudio.com/extensions.html">extensions</a> that call the full Spark API and provide <br/> interfaces to Spark packages.</li>
</ul>

<h2 id="installation">Installation</h2>

<p>You can install the <strong>sparklyr</strong> package from CRAN as follows:</p>

<pre><code class="language-r">install.packages(&quot;sparklyr&quot;)
</code></pre>

<p>You should also install a local version of Spark for development purposes:</p>

<pre><code class="language-r">library(sparklyr)
spark_install(version = &quot;2.1.0&quot;)
</code></pre>

<p>To upgrade to the latest version of sparklyr, run the following command and restart your r session:</p>

<pre><code class="language-r">devtools::install_github(&quot;rstudio/sparklyr&quot;)
</code></pre>

<p>If you use the RStudio IDE, you should also download the latest <a href="https://www.rstudio.com/products/rstudio/download/preview/">preview release</a> of the IDE which includes several enhancements for interacting with Spark (see the <a href="#rstudio-ide">RStudio IDE</a> section below for more details).</p>

<h2 id="connecting-to-spark">Connecting to Spark</h2>

<p>You can connect to both local instances of Spark as well as remote Spark clusters. Here we&rsquo;ll connect to a local instance of Spark via the <a href="http://spark.rstudio.com/reference/sparklyr/latest/spark_connect.html">spark_connect</a> function:</p>

<pre><code class="language-r">library(sparklyr)
sc &lt;- spark_connect(master = &quot;local&quot;)
</code></pre>

<p>The returned Spark connection (<code>sc</code>) provides a remote dplyr data source to the Spark cluster.</p>

<p>For more information on connecting to remote Spark clusters see the <a href="http://spark.rstudio.com/deployment.html">Deployment</a> section of the sparklyr website.</p>

<h2 id="using-dplyr">Using dplyr</h2>

<p>We can now use all of the available dplyr verbs against the tables within the cluster.</p>

<p>We&rsquo;ll start by copying some datasets from R into the Spark cluster (note that you may need to install the nycflights13 and Lahman packages in order to execute this code):</p>

<pre><code class="language-r">install.packages(c(&quot;nycflights13&quot;, &quot;Lahman&quot;))
</code></pre>

<pre><code class="language-r">library(dplyr)
iris_tbl &lt;- copy_to(sc, iris)
flights_tbl &lt;- copy_to(sc, nycflights13::flights, &quot;flights&quot;)
batting_tbl &lt;- copy_to(sc, Lahman::Batting, &quot;batting&quot;)
src_tbls(sc)
</code></pre>

<pre><code>## [1] &quot;batting&quot; &quot;flights&quot; &quot;iris&quot;
</code></pre>

<p>To start with here&rsquo;s a simple filtering example:</p>

<pre><code class="language-r"># filter by departure delay and print the first few records
flights_tbl %&gt;% filter(dep_delay == 2)
</code></pre>

<pre><code>## # Source:   lazy query [?? x 19]
## # Database: spark_connection
##     year month   day dep_time sched_dep_time dep_delay arr_time
##    &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;
##  1  2013     1     1      517            515         2      830
##  2  2013     1     1      542            540         2      923
##  3  2013     1     1      702            700         2     1058
##  4  2013     1     1      715            713         2      911
##  5  2013     1     1      752            750         2     1025
##  6  2013     1     1      917            915         2     1206
##  7  2013     1     1      932            930         2     1219
##  8  2013     1     1     1028           1026         2     1350
##  9  2013     1     1     1042           1040         2     1325
## 10  2013     1     1     1231           1229         2     1523
## # ... with more rows, and 12 more variables: sched_arr_time &lt;int&gt;,
## #   arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;,
## #   origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;,
## #   minute &lt;dbl&gt;, time_hour &lt;dbl&gt;
</code></pre>

<p><a href="https://CRAN.R-project.org/package=dplyr">Introduction to dplyr</a> provides additional dplyr examples you can try. For example, consider the last example from the tutorial which plots data on flight delays:</p>

<pre><code class="language-r">delay &lt;- flights_tbl %&gt;% 
  group_by(tailnum) %&gt;%
  summarise(count = n(), dist = mean(distance), delay = mean(arr_delay)) %&gt;%
  filter(count &gt; 20, dist &lt; 2000, !is.na(delay)) %&gt;%
  collect

# plot delays
library(ggplot2)
ggplot(delay, aes(dist, delay)) +
  geom_point(aes(size = count), alpha = 1/2) +
  geom_smooth() +
  scale_size_area(max_size = 2)
</code></pre>

<pre><code>## `geom_smooth()` using method = 'gam'
</code></pre>

<p><img src="tools/readme/ggplot2-1.png" alt="" /></p>

<h3 id="window-functions">Window Functions</h3>

<p>dplyr <a href="https://CRAN.R-project.org/package=dplyr">window functions</a> are also supported, for example:</p>

<pre><code class="language-r">batting_tbl %&gt;%
  select(playerID, yearID, teamID, G, AB:H) %&gt;%
  arrange(playerID, yearID, teamID) %&gt;%
  group_by(playerID) %&gt;%
  filter(min_rank(desc(H)) &lt;= 2 &amp; H &gt; 0)
</code></pre>

<pre><code>## # Source:     lazy query [?? x 7]
## # Database:   spark_connection
## # Groups:     playerID
## # Ordered by: playerID, yearID, teamID
##     playerID yearID teamID     G    AB     R     H
##        &lt;chr&gt;  &lt;int&gt;  &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;
##  1 aaronha01   1959    ML1   154   629   116   223
##  2 aaronha01   1963    ML1   161   631   121   201
##  3 abbotji01   1999    MIL    20    21     0     2
##  4 abnersh01   1992    CHA    97   208    21    58
##  5 abnersh01   1990    SDN    91   184    17    45
##  6 acklefr01   1963    CHA     2     5     0     1
##  7 acklefr01   1964    CHA     3     1     0     1
##  8 adamecr01   2016    COL   121   225    25    49
##  9 adamecr01   2015    COL    26    53     4    13
## 10 adamsac01   1943    NY1    70    32     3     4
## # ... with more rows
</code></pre>

<p>For additional documentation on using dplyr with Spark see the <a href="http://spark.rstudio.com/dplyr.html">dplyr</a> section of the sparklyr website.</p>

<h2 id="using-sql">Using SQL</h2>

<p>It&rsquo;s also possible to execute SQL queries directly against tables within a Spark cluster. The <code>spark_connection</code> object implements a <a href="https://github.com/rstats-db/DBI">DBI</a> interface for Spark, so you can use <code>dbGetQuery</code> to execute SQL and return the result as an R data frame:</p>

<pre><code class="language-r">library(DBI)
iris_preview &lt;- dbGetQuery(sc, &quot;SELECT * FROM iris LIMIT 10&quot;)
iris_preview
</code></pre>

<pre><code>##    Sepal_Length Sepal_Width Petal_Length Petal_Width Species
## 1           5.1         3.5          1.4         0.2  setosa
## 2           4.9         3.0          1.4         0.2  setosa
## 3           4.7         3.2          1.3         0.2  setosa
## 4           4.6         3.1          1.5         0.2  setosa
## 5           5.0         3.6          1.4         0.2  setosa
## 6           5.4         3.9          1.7         0.4  setosa
## 7           4.6         3.4          1.4         0.3  setosa
## 8           5.0         3.4          1.5         0.2  setosa
## 9           4.4         2.9          1.4         0.2  setosa
## 10          4.9         3.1          1.5         0.1  setosa
</code></pre>

<h2 id="machine-learning">Machine Learning</h2>

<p>You can orchestrate machine learning algorithms in a Spark cluster via the <a href="http://spark.apache.org/docs/latest/mllib-guide.html">machine learning</a> functions within <strong>sparklyr</strong>. These functions connect to a set of high-level APIs built on top of DataFrames that help you create and tune machine learning workflows.</p>

<p>Here&rsquo;s an example where we use <a href="http://spark.rstudio.com/reference/sparklyr/latest/ml_linear_regression.html">ml_linear_regression</a> to fit a linear regression model. We&rsquo;ll use the built-in <code>mtcars</code> dataset, and see if we can predict a car&rsquo;s fuel consumption (<code>mpg</code>) based on its weight (<code>wt</code>), and the number of cylinders the engine contains (<code>cyl</code>). We&rsquo;ll assume in each case that the relationship between <code>mpg</code> and each of our features is linear.</p>

<pre><code class="language-r"># copy mtcars into spark
mtcars_tbl &lt;- copy_to(sc, mtcars)

# transform our data set, and then partition into 'training', 'test'
partitions &lt;- mtcars_tbl %&gt;%
  filter(hp &gt;= 100) %&gt;%
  mutate(cyl8 = cyl == 8) %&gt;%
  sdf_partition(training = 0.5, test = 0.5, seed = 1099)

# fit a linear model to the training dataset
fit &lt;- partitions$training %&gt;%
  ml_linear_regression(response = &quot;mpg&quot;, features = c(&quot;wt&quot;, &quot;cyl&quot;))
</code></pre>

<pre><code>## * No rows dropped by 'na.omit' call
</code></pre>

<pre><code class="language-r">fit
</code></pre>

<pre><code>## Call: ml_linear_regression(., response = &quot;mpg&quot;, features = c(&quot;wt&quot;, &quot;cyl&quot;))
## 
## Coefficients:
## (Intercept)          wt         cyl 
##   33.499452   -2.818463   -0.923187
</code></pre>

<p>For linear regression models produced by Spark, we can use <code>summary()</code> to learn a bit more about the quality of our fit, and the statistical significance of each of our predictors.</p>

<pre><code class="language-r">summary(fit)
</code></pre>

<pre><code>## Call: ml_linear_regression(., response = &quot;mpg&quot;, features = c(&quot;wt&quot;, &quot;cyl&quot;))
## 
## Deviance Residuals::
##    Min     1Q Median     3Q    Max 
## -1.752 -1.134 -0.499  1.296  2.282 
## 
## Coefficients:
##             Estimate Std. Error t value  Pr(&gt;|t|)    
## (Intercept) 33.49945    3.62256  9.2475 0.0002485 ***
## wt          -2.81846    0.96619 -2.9171 0.0331257 *  
## cyl         -0.92319    0.54639 -1.6896 0.1518998    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## R-Squared: 0.8274
## Root Mean Squared Error: 1.422
</code></pre>

<p>Spark machine learning supports a wide array of algorithms and feature transformations and as illustrated above it&rsquo;s easy to chain these functions together with dplyr pipelines. To learn more see the <a href="https://github.com/rstudio/sparklyr/blob/master/docs/articles/mllib.html">machine learning</a> section.</p>

<h2 id="reading-and-writing-data">Reading and Writing Data</h2>

<p>You can read and write data in CSV, JSON, and Parquet formats. Data can be stored in HDFS, S3, or on the local filesystem of cluster nodes.</p>

<pre><code class="language-r">temp_csv &lt;- tempfile(fileext = &quot;.csv&quot;)
temp_parquet &lt;- tempfile(fileext = &quot;.parquet&quot;)
temp_json &lt;- tempfile(fileext = &quot;.json&quot;)

spark_write_csv(iris_tbl, temp_csv)
iris_csv_tbl &lt;- spark_read_csv(sc, &quot;iris_csv&quot;, temp_csv)

spark_write_parquet(iris_tbl, temp_parquet)
iris_parquet_tbl &lt;- spark_read_parquet(sc, &quot;iris_parquet&quot;, temp_parquet)

spark_write_json(iris_tbl, temp_json)
iris_json_tbl &lt;- spark_read_json(sc, &quot;iris_json&quot;, temp_json)

src_tbls(sc)
</code></pre>

<pre><code>## [1] &quot;batting&quot;      &quot;flights&quot;      &quot;iris&quot;         &quot;iris_csv&quot;    
## [5] &quot;iris_json&quot;    &quot;iris_parquet&quot; &quot;mtcars&quot;
</code></pre>

<h2 id="distributed-r">Distributed R</h2>

<p>You can execute arbitrary r code across your cluster using <code>spark_apply</code>. For example, we can apply <code>rgamma</code> over <code>iris</code> as follows:</p>

<pre><code class="language-r">spark_apply(iris_tbl, function(data) {
  data[1:4] + rgamma(1,2)
})
</code></pre>

<pre><code>## # Source:   table&lt;sparklyr_tmp_117d835ce14f9&gt; [?? x 4]
## # Database: spark_connection
##    Sepal_Length Sepal_Width Petal_Length Petal_Width
##           &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt;
##  1     9.983684    8.383684     6.283684    5.083684
##  2     9.783684    7.883684     6.283684    5.083684
##  3     9.583684    8.083684     6.183684    5.083684
##  4     9.483684    7.983684     6.383684    5.083684
##  5     9.883684    8.483684     6.283684    5.083684
##  6    10.283684    8.783684     6.583684    5.283684
##  7     9.483684    8.283684     6.283684    5.183684
##  8     9.883684    8.283684     6.383684    5.083684
##  9     9.283684    7.783684     6.283684    5.083684
## 10     9.783684    7.983684     6.383684    4.983684
## # ... with more rows
</code></pre>

<p>You can also group by columns to perform an operation over each group of rows and make use of any package within the closure:</p>

<pre><code class="language-r">spark_apply(
  iris_tbl,
  function(e) broom::tidy(lm(Petal_Width ~ Petal_Length, e)),
  names = c(&quot;term&quot;, &quot;estimate&quot;, &quot;std.error&quot;, &quot;statistic&quot;, &quot;p.value&quot;),
  group_by = &quot;Species&quot;
)
</code></pre>

<pre><code>## # Source:   table&lt;sparklyr_tmp_117d876fbf859&gt; [?? x 6]
## # Database: spark_connection
##      Species         term    estimate  std.error  statistic      p.value
##        &lt;chr&gt;        &lt;chr&gt;       &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;        &lt;dbl&gt;
## 1 versicolor  (Intercept) -0.08428835 0.16070140 -0.5245029 6.023428e-01
## 2 versicolor Petal_Length  0.33105360 0.03750041  8.8279995 1.271916e-11
## 3  virginica  (Intercept)  1.13603130 0.37936622  2.9945505 4.336312e-03
## 4  virginica Petal_Length  0.16029696 0.06800119  2.3572668 2.253577e-02
## 5     setosa  (Intercept) -0.04822033 0.12164115 -0.3964146 6.935561e-01
## 6     setosa Petal_Length  0.20124509 0.08263253  2.4354220 1.863892e-02
</code></pre>

<h2 id="extensions">Extensions</h2>

<p>The facilities used internally by sparklyr for its dplyr and machine learning interfaces are available to extension packages. Since Spark is a general purpose cluster computing system there are many potential applications for extensions (e.g. interfaces to custom machine learning pipelines, interfaces to 3rd party Spark packages, etc.).</p>

<p>Here&rsquo;s a simple example that wraps a Spark text file line counting function with an R function:</p>

<pre><code class="language-r"># write a CSV 
tempfile &lt;- tempfile(fileext = &quot;.csv&quot;)
write.csv(nycflights13::flights, tempfile, row.names = FALSE, na = &quot;&quot;)

# define an R interface to Spark line counting
count_lines &lt;- function(sc, path) {
  spark_context(sc) %&gt;% 
    invoke(&quot;textFile&quot;, path, 1L) %&gt;% 
      invoke(&quot;count&quot;)
}

# call spark to count the lines of the CSV
count_lines(sc, tempfile)
</code></pre>

<pre><code>## [1] 336777
</code></pre>

<p>To learn more about creating extensions see the <a href="http://spark.rstudio.com/extensions.html">Extensions</a> section of the sparklyr website.</p>

<h2 id="table-utilities">Table Utilities</h2>

<p>You can cache a table into memory with:</p>

<pre><code class="language-r">tbl_cache(sc, &quot;batting&quot;)
</code></pre>

<p>and unload from memory using:</p>

<pre><code class="language-r">tbl_uncache(sc, &quot;batting&quot;)
</code></pre>

<h2 id="connection-utilities">Connection Utilities</h2>

<p>You can view the Spark web console using the <code>spark_web</code> function:</p>

<pre><code class="language-r">spark_web(sc)
</code></pre>

<p>You can show the log using the <code>spark_log</code> function:</p>

<pre><code class="language-r">spark_log(sc, n = 10)
</code></pre>

<pre><code>## 17/08/25 12:54:28 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 72 (/var/folders/vd/krh_y3qd0c5bw8k77lmdtw7r0000gn/T//Rtmps6VbEg/file117d8538598e1.csv MapPartitionsRDD[293] at textFile at NativeMethodAccessorImpl.java:0)
## 17/08/25 12:54:28 INFO TaskSchedulerImpl: Adding task set 72.0 with 1 tasks
## 17/08/25 12:54:28 INFO TaskSetManager: Starting task 0.0 in stage 72.0 (TID 112, localhost, executor driver, partition 0, PROCESS_LOCAL, 6010 bytes)
## 17/08/25 12:54:28 INFO Executor: Running task 0.0 in stage 72.0 (TID 112)
## 17/08/25 12:54:28 INFO HadoopRDD: Input split: file:/var/folders/vd/krh_y3qd0c5bw8k77lmdtw7r0000gn/T/Rtmps6VbEg/file117d8538598e1.csv:0+33313106
## 17/08/25 12:54:28 INFO Executor: Finished task 0.0 in stage 72.0 (TID 112). 1123 bytes result sent to driver
## 17/08/25 12:54:28 INFO TaskSetManager: Finished task 0.0 in stage 72.0 (TID 112) in 114 ms on localhost (executor driver) (1/1)
## 17/08/25 12:54:28 INFO TaskSchedulerImpl: Removed TaskSet 72.0, whose tasks have all completed, from pool 
## 17/08/25 12:54:28 INFO DAGScheduler: ResultStage 72 (count at NativeMethodAccessorImpl.java:0) finished in 0.114 s
## 17/08/25 12:54:28 INFO DAGScheduler: Job 49 finished: count at NativeMethodAccessorImpl.java:0, took 0.117630 s
</code></pre>

<p>Finally, we disconnect from Spark:</p>

<pre><code class="language-r">spark_disconnect(sc)
</code></pre>

<h2 id="rstudio-ide">RStudio IDE</h2>

<p>The latest RStudio <a href="https://www.rstudio.com/products/rstudio/download/preview/">Preview Release</a> of the RStudio IDE includes integrated support for Spark and the sparklyr package, including tools for:</p>

<ul>
<li>Creating and managing Spark connections</li>
<li>Browsing the tables and columns of Spark DataFrames</li>
<li>Previewing the first 1,000 rows of Spark DataFrames</li>
</ul>

<p>Once you&rsquo;ve installed the sparklyr package, you should find a new <strong>Spark</strong> pane within the IDE. This pane includes a <strong>New Connection</strong> dialog which can be used to make connections to local or remote Spark instances:</p>

<p><img src="tools/readme/spark-connect.png" class="screenshot" width=389 /></p>

<p>Once you&rsquo;ve connected to Spark you&rsquo;ll be able to browse the tables contained within the Spark cluster and preview Spark DataFrames using the standard RStudio data viewer:</p>

<p><img src="tools/readme/spark-dataview.png" class="screenshot" width=639 /></p>

<p>You can also connect to Spark through <a href="http://livy.io">Livy</a> through a new connection dialog:</p>

<p><img src="tools/readme/spark-connect-livy.png" class="screenshot" width=389 /></p>

<p>The RStudio IDE features for sparklyr are available now as part of the <a href="https://www.rstudio.com/products/rstudio/download/preview/">RStudio Preview Release</a>.</p>

<h2 id="using-h2o">Using H2O</h2>

<p><a href="https://cran.r-project.org/package=rsparkling">rsparkling</a> is a CRAN package from <a href="http://h2o.ai">H2O</a> that extends <a href="http://spark.rstudio.com">sparklyr</a> to provide an interface into <a href="https://github.com/h2oai/sparkling-water">Sparkling Water</a>. For instance, the following example installs, configures and runs <a href="http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/glm.html">h2o.glm</a>:</p>

<pre><code class="language-r">options(rsparkling.sparklingwater.version = &quot;2.1.0&quot;)

library(rsparkling)
library(sparklyr)
library(dplyr)
library(h2o)

sc &lt;- spark_connect(master = &quot;local&quot;, version = &quot;2.1.0&quot;)
mtcars_tbl &lt;- copy_to(sc, mtcars, &quot;mtcars&quot;)

mtcars_h2o &lt;- as_h2o_frame(sc, mtcars_tbl, strict_version_check = FALSE)

mtcars_glm &lt;- h2o.glm(x = c(&quot;wt&quot;, &quot;cyl&quot;), 
                      y = &quot;mpg&quot;,
                      training_frame = mtcars_h2o,
                      lambda_search = TRUE)
</code></pre>

<pre><code class="language-r">mtcars_glm
</code></pre>

<pre><code>## Model Details:
## ==============
## 
## H2ORegressionModel: glm
## Model ID:  GLM_model_R_1503683692912_1 
## GLM Model: summary
##     family     link                              regularization
## 1 gaussian identity Elastic Net (alpha = 0.5, lambda = 0.1013 )
##                                                                lambda_search
## 1 nlambda = 100, lambda.max = 10.132, lambda.min = 0.1013, lambda.1se = -1.0
##   number_of_predictors_total number_of_active_predictors
## 1                          2                           2
##   number_of_iterations                                training_frame
## 1                    0 frame_rdd_29_970371551cefadb7219d3e25e94a4bc0
## 
## Coefficients: glm coefficients
##       names coefficients standardized_coefficients
## 1 Intercept    38.941654                 20.090625
## 2       cyl    -1.468783                 -2.623132
## 3        wt    -3.034558                 -2.969186
## 
## H2ORegressionMetrics: glm
## ** Reported on training data. **
## 
## MSE:  6.017684
## RMSE:  2.453097
## MAE:  1.940985
## RMSLE:  0.1114801
## Mean Residual Deviance :  6.017684
## R^2 :  0.8289895
## Null Deviance :1126.047
## Null D.o.F. :31
## Residual Deviance :192.5659
## Residual D.o.F. :29
## AIC :156.2425
</code></pre>

<pre><code class="language-r">spark_disconnect(sc)
</code></pre>

<h2 id="connecting-through-livy">Connecting through Livy</h2>

<p><a href="https://github.com/cloudera/livy">Livy</a> enables remote connections to Apache Spark clusters. Connecting to Spark clusters through Livy is <strong>under experimental development</strong> in <code>sparklyr</code>. Please post any feedback or questions as a GitHub issue as needed.</p>

<p>Before connecting to Livy, you will need the connection information to an existing service running Livy. Otherwise, to test <code>livy</code> in your local environment, you can install it and run it locally as follows:</p>

<pre><code class="language-r">livy_install()
</code></pre>

<pre><code class="language-r">livy_service_start()
</code></pre>

<p>To connect, use the Livy service address as <code>master</code> and <code>method = &quot;livy&quot;</code> in <code>spark_connect</code>. Once connection completes, use <code>sparklyr</code> as usual, for instance:</p>

<pre><code class="language-r">sc &lt;- spark_connect(master = &quot;http://localhost:8998&quot;, method = &quot;livy&quot;)
copy_to(sc, iris)
</code></pre>

<pre><code>## # Source:   table&lt;iris&gt; [?? x 5]
## # Database: spark_connection
##    Sepal_Length Sepal_Width Petal_Length Petal_Width Species
##           &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt;   &lt;chr&gt;
##  1          5.1         3.5          1.4         0.2  setosa
##  2          4.9         3.0          1.4         0.2  setosa
##  3          4.7         3.2          1.3         0.2  setosa
##  4          4.6         3.1          1.5         0.2  setosa
##  5          5.0         3.6          1.4         0.2  setosa
##  6          5.4         3.9          1.7         0.4  setosa
##  7          4.6         3.4          1.4         0.3  setosa
##  8          5.0         3.4          1.5         0.2  setosa
##  9          4.4         2.9          1.4         0.2  setosa
## 10          4.9         3.1          1.5         0.1  setosa
## # ... with more rows
</code></pre>

<pre><code class="language-r">spark_disconnect(sc)
</code></pre>

<p>Once you are done using <code>livy</code> locally, you should stop this service with:</p>

<pre><code class="language-r">livy_service_stop()
</code></pre>

<p>To connect to remote <code>livy</code> clusters that support basic authentication connect as:</p>

<pre><code class="language-r">config &lt;- livy_config_auth(&quot;&lt;username&gt;&quot;, &quot;&lt;password&quot;&gt;)
sc &lt;- spark_connect(master = &quot;&lt;address&gt;&quot;, method = &quot;livy&quot;, config = config)
spark_disconnect(sc)
</code></pre>

        
      </div>
      </div>
      </div>
      
      
<footer>



</footer>
      
    </div>
</div>

</body>
</html>

