<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Distributing R Computations • sparklyr</title>
<!-- jquery --><script src="https://code.jquery.com/jquery-3.1.0.min.js" integrity="sha384-nrOSfDHtoPMzJHjVTdCopGqIqeYETSXhZDFyniQ8ZHcVy08QesyHcnOUpMpqnmWq" crossorigin="anonymous"></script><!-- Bootstrap --><link href="https://maxcdn.bootstrapcdn.com/bootswatch/3.3.7/cosmo/bootstrap.min.css" rel="stylesheet" crossorigin="anonymous">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script><!-- Font Awesome icons --><link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" integrity="sha384-T8Gy5hrqNKT+hzMclPo118YTQO6cYprQmhrYwIiQ/3axmI1hQomh7Ud2hPOy8SP1" crossorigin="anonymous">
<!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../jquery.sticky-kit.min.js"></script><script src="../pkgdown.js"></script><!-- mathjax --><script src="https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <div class="container template-vignette">
      <header><div class="navbar navbar-inverse navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="../index.html">sparklyr</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../index.html">Home</a>
</li>
<li>
  <a href="../articles/gallery.html">Gallery</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Guides
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../articles/guides-dplyr.html">dplyr</a>
    </li>
    <li>
      <a href="../articles/guides-mllib.html">mllib</a>
    </li>
    <li>
      <a href="../articles/guides-h2o.html">H2O</a>
    </li>
    <li>
      <a href="../articles/guides-extensions.html">Extensions</a>
    </li>
    <li>
      <a href="../articles/guides-caching.html">Caching</a>
    </li>
    <li>
      <a href="../articles/guides-distributed-r.html">Distributed R</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Deployment
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../articles/deployment-overview.html">Overview</a>
    </li>
    <li>
      <a href="../articles/deployment-data-lakes.html">Data Lakes</a>
    </li>
    <li>
      <a href="../articles/deployment-cdh.html">Cloudera</a>
    </li>
    <li>
      <a href="../articles/deployment-amazon.html">Amazon</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Reference
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../reference/index.html">Functions</a>
    </li>
    <li>
      <a href="https://github.com/rstudio/cheatsheets/raw/master/source/pdfs/sparklyr.pdf">Cheatsheet</a>
    </li>
    <li>
      <a href="../articles/reference-media.html">Media</a>
    </li>
    <li>
      <a href="../news/index.html">News</a>
    </li>
  </ul>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="https://github.com/rstudio/reticulate">
    <span class="fa fa-github"></span>
     
  </a>
</li>
      </ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      
      </header><div class="row">
  <div class="col-md-9">
    <div class="page-header toc-ignore">
      <h1>Distributing R Computations</h1>
            
          </div>

    
    
<div class="contents">
<div id="overview" class="section level2">
<h2 class="hasAnchor">
<a href="#overview" class="anchor"></a>Overview</h2>
<p><strong>sparklyr</strong> provides support to run arbitrary R code at scale within your Spark Cluster through <code><a href="../reference/spark_apply.html">spark_apply()</a></code>. This is especially useful where there is a need to use functionality available only in R or R packages that is not available in Apache Spark nor <a href="https://spark-packages.org/">Spark Packages</a>.</p>
<p><code><a href="../reference/spark_apply.html">spark_apply()</a></code> applies an R function to a Spark object (typically, a Spark DataFrame). Spark objects are partitioned so they can be distributed across a cluster. You can use <code>spark_apply</code> with the default partitions or you can define your own partitions with the <code>group_by</code> argument. Your R function must return another Spark DataFrame. <code>spark_apply</code> will run your R function on each partition and output a single Spark DataFrame.</p>
<div id="apply-an-r-function-to-a-spark-object" class="section level3">
<h3 class="hasAnchor">
<a href="#apply-an-r-function-to-a-spark-object" class="anchor"></a>Apply an R function to a Spark Object</h3>
<p>Lets run a simple example. We will apply the identify function, <code>I()</code>, over a list of numbers we created with the <code>sdf_len</code> function.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(sparklyr)

sc &lt;-<span class="st"> </span><span class="kw"><a href="../reference/spark-connections.html">spark_connect</a></span>(<span class="dt">master =</span> <span class="st">"local"</span>)

<span class="kw"><a href="../reference/sdf_len.html">sdf_len</a></span>(sc, <span class="dv">5</span>, <span class="dt">repartition =</span> <span class="dv">1</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw"><a href="../reference/spark_apply.html">spark_apply</a></span>(<span class="cf">function</span>(e) <span class="kw">I</span>(e))</code></pre></div>
<pre><code>## # Source:   table&lt;sparklyr_tmp_1b5b1ed4422b&gt; [?? x 1]
## # Database: spark_connection
##      id
##   &lt;dbl&gt;
## 1     1
## 2     2
## 3     3
## 4     4
## 5     5</code></pre>
<p>Your R function should be designed to operate on an R <a href="https://stat.ethz.ch/R-manual/R-devel/library/base/html/data.frame.html">data frame</a>. The R function passed to <code>spark_apply</code> expects a DataFrame and will return an object that can be cast as a DataFrame. We can use the <code>class</code> function to verify the class of the data.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw"><a href="../reference/sdf_len.html">sdf_len</a></span>(sc, <span class="dv">10</span>, <span class="dt">repartition =</span> <span class="dv">1</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw"><a href="../reference/spark_apply.html">spark_apply</a></span>(<span class="cf">function</span>(e) <span class="kw">class</span>(e))</code></pre></div>
<pre><code>## # Source:   table&lt;sparklyr_tmp_1b5b4269692b&gt; [?? x 1]
## # Database: spark_connection
##           id
##        &lt;chr&gt;
## 1 data.frame</code></pre>
<p>Spark will partition your data by hash or range so it can be distributed across a cluster. In the following example we create two partitions and count the number of rows in each partition. Then we print the first record in each partition.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">trees_tbl &lt;-<span class="st"> </span><span class="kw">sdf_copy_to</span>(sc, trees, <span class="dt">repartition =</span> <span class="dv">2</span>)

trees_tbl <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw"><a href="../reference/spark_apply.html">spark_apply</a></span>(<span class="cf">function</span>(e) <span class="kw">nrow</span>(e), <span class="dt">names =</span> <span class="st">"n"</span>)</code></pre></div>
<pre><code>## # Source:   table&lt;sparklyr_tmp_1b5b9be1371&gt; [?? x 1]
## # Database: spark_connection
##       n
##   &lt;int&gt;
## 1    16
## 2    15</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">trees_tbl <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw"><a href="../reference/spark_apply.html">spark_apply</a></span>(<span class="cf">function</span>(e) <span class="kw">head</span>(e, <span class="dv">1</span>))</code></pre></div>
<pre><code>## # Source:   table&lt;sparklyr_tmp_1b5b1dee64b6&gt; [?? x 3]
## # Database: spark_connection
##   Girth Height Volume
##   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;
## 1   8.3     70   10.3
## 2   8.6     65   10.3</code></pre>
<p>We can apply any arbitrary function to the partitions in the Spark DataFrame. For instance, we can scale or jitter the columns. Notice that <code>spark_apply</code> applies the R function to all partitions and returns a single DataFrame.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">trees_tbl <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw"><a href="../reference/spark_apply.html">spark_apply</a></span>(<span class="cf">function</span>(e) <span class="kw">scale</span>(e))</code></pre></div>
<pre><code>## # Source:   table&lt;sparklyr_tmp_1b5b4451f045&gt; [?? x 3]
## # Database: spark_connection
##         Girth      Height     Volume
##         &lt;dbl&gt;       &lt;dbl&gt;      &lt;dbl&gt;
##  1 -1.4482330 -0.99510521 -1.1503645
##  2 -1.3021313 -2.06675697 -1.1558670
##  3 -0.7469449  0.68891899 -0.6826528
##  4 -0.6592839 -1.60747764 -0.8587325
##  5 -0.6300635  0.53582588 -0.4735581
##  6 -0.5716229  0.38273277 -0.3855183
##  7 -0.5424025 -0.07654655 -0.5395880
##  8 -0.3670805 -0.22963966 -0.6661453
##  9 -0.1040975  1.30129143  0.1427209
## 10  0.1296653 -0.84201210 -0.3029809
## # ... with 21 more rows</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">trees_tbl <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw"><a href="../reference/spark_apply.html">spark_apply</a></span>(<span class="cf">function</span>(e) <span class="kw">lapply</span>(e, jitter))</code></pre></div>
<pre><code>## # Source:   table&lt;sparklyr_tmp_1b5b5f75650d&gt; [?? x 3]
## # Database: spark_connection
##        Girth   Height   Volume
##        &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;
##  1  8.300609 70.06614 10.31927
##  2  8.782202 63.14384 10.20913
##  3 10.705198 80.85294 18.78867
##  4 10.998014 66.08903 15.61699
##  5 11.096001 80.09383 22.59187
##  6 11.295074 79.01472 24.19148
##  7 11.398111 76.09978 21.39239
##  8 11.994976 74.98846 19.08380
##  9 12.910685 85.03434 33.80400
## 10 13.711526 70.80274 25.70250
## # ... with 21 more rows</code></pre>
<p>By default <code><a href="../reference/spark_apply.html">spark_apply()</a></code> derives the column names from the input Spark data frame. Use the <code>names</code> argument to rename or add new columns.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">trees_tbl <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw"><a href="../reference/spark_apply.html">spark_apply</a></span>(
    <span class="cf">function</span>(e) <span class="kw">data.frame</span>(<span class="fl">2.54</span> <span class="op">*</span><span class="st"> </span>e<span class="op">$</span>Girth, e),
    <span class="dt">names =</span> <span class="kw">c</span>(<span class="st">"Girth(cm)"</span>, <span class="kw">colnames</span>(trees)))</code></pre></div>
<pre><code>## # Source:   table&lt;sparklyr_tmp_1b5b4bd58418&gt; [?? x 4]
## # Database: spark_connection
##    `Girth(cm)` Girth Height Volume
##          &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;
##  1      21.082   8.3     70   10.3
##  2      22.352   8.8     63   10.2
##  3      27.178  10.7     81   18.8
##  4      27.940  11.0     66   15.6
##  5      28.194  11.1     80   22.6
##  6      28.702  11.3     79   24.2
##  7      28.956  11.4     76   21.4
##  8      30.480  12.0     75   19.1
##  9      32.766  12.9     85   33.8
## 10      34.798  13.7     71   25.7
## # ... with 21 more rows</code></pre>
</div>
<div id="group-by" class="section level3">
<h3 class="hasAnchor">
<a href="#group-by" class="anchor"></a>Group By</h3>
<p>In some cases you may want to apply your R function to specific groups in your data. For example, suppose you want to compute regression models against specific subgroups. To solve this, you can specify a <code>group_by</code> argument. This example counts the number of rows in <code>iris</code> by species and then fits a simple linear model for each species.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">iris_tbl &lt;-<span class="st"> </span><span class="kw">sdf_copy_to</span>(sc, iris)

iris_tbl <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw"><a href="../reference/spark_apply.html">spark_apply</a></span>(nrow, <span class="dt">group_by =</span> <span class="st">"Species"</span>)</code></pre></div>
<pre><code>## # Source:   table&lt;sparklyr_tmp_1b5b39f866ff&gt; [?? x 2]
## # Database: spark_connection
##      Species Sepal_Length
##        &lt;chr&gt;        &lt;int&gt;
## 1 versicolor           50
## 2  virginica           50
## 3     setosa           50</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">iris_tbl <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw"><a href="../reference/spark_apply.html">spark_apply</a></span>(
    <span class="cf">function</span>(e) <span class="kw">summary</span>(<span class="kw">lm</span>(Petal_Length <span class="op">~</span><span class="st"> </span>Petal_Width, e))<span class="op">$</span>r.squared,
    <span class="dt">names =</span> <span class="st">"r.squared"</span>,
    <span class="dt">group_by =</span> <span class="st">"Species"</span>)</code></pre></div>
<pre><code>## # Source:   table&lt;sparklyr_tmp_1b5b63320d14&gt; [?? x 2]
## # Database: spark_connection
##      Species r.squared
##        &lt;chr&gt;     &lt;dbl&gt;
## 1 versicolor 0.6188467
## 2  virginica 0.1037537
## 3     setosa 0.1099785</code></pre>
</div>
</div>
<div id="distributing-packages" class="section level2">
<h2 class="hasAnchor">
<a href="#distributing-packages" class="anchor"></a>Distributing Packages</h2>
<p>With <code><a href="../reference/spark_apply.html">spark_apply()</a></code> you can use any R package inside Spark. For instance, you can use the <a href="https://cran.r-project.org/package=broom">broom</a> package to create a tidy data frame from linear regression output.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw"><a href="../reference/spark_apply.html">spark_apply</a></span>(
  iris_tbl,
  <span class="cf">function</span>(e) broom<span class="op">::</span><span class="kw"><a href="http://www.rdocumentation.org/packages/broom/topics/tidy">tidy</a></span>(<span class="kw">lm</span>(Petal_Length <span class="op">~</span><span class="st"> </span>Petal_Width, e)),
  <span class="dt">names =</span> <span class="kw">c</span>(<span class="st">"term"</span>, <span class="st">"estimate"</span>, <span class="st">"std.error"</span>, <span class="st">"statistic"</span>, <span class="st">"p.value"</span>),
  <span class="dt">group_by =</span> <span class="st">"Species"</span>)</code></pre></div>
<pre><code>## # Source:   table&lt;sparklyr_tmp_1b5b74d91ed1&gt; [?? x 6]
## # Database: spark_connection
##      Species        term  estimate std.error statistic      p.value
##        &lt;chr&gt;       &lt;chr&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;
## 1 versicolor (Intercept) 1.7812754 0.2838234  6.276000 9.484134e-08
## 2 versicolor Petal_Width 1.8693247 0.2117495  8.827999 1.271916e-11
## 3  virginica (Intercept) 4.2406526 0.5612870  7.555230 1.041600e-09
## 4  virginica Petal_Width 0.6472593 0.2745804  2.357267 2.253577e-02
## 5     setosa (Intercept) 1.3275634 0.0599594 22.141037 7.676120e-27
## 6     setosa Petal_Width 0.5464903 0.2243924  2.435422 1.863892e-02</code></pre>
<p>To use R packages inside Spark, your packages must be installed on the worker nodes. The first time you call <code>spark_apply</code> all of the contents in your local <code>.libPaths()</code> will be copied into each Spark worker node via the <code>SparkConf.addFile()</code> function. Packages will only be copied once and will persist as long as the connection remains open. It’s not uncommon for R libraries to be several gigabytes in size, so be prepared for a one-time tax while the R packages are copied over to your Spark cluster. You can disable package distribution by setting <code>packages = FALSE</code>. Note: packages are not copied in local mode (<code>master="local"</code>) because the packages already exist on the system.</p>
</div>
<div id="handling-errors" class="section level2">
<h2 class="hasAnchor">
<a href="#handling-errors" class="anchor"></a>Handling Errors</h2>
<p>It can be more difficult to troubleshoot R issues in a cluster than in local mode. For instance, the following R code causes the distributed execution to fail and suggests you check the logs for details.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw"><a href="../reference/spark_apply.html">spark_apply</a></span>(iris_tbl, <span class="cf">function</span>(e) <span class="kw">stop</span>(<span class="st">"Make this fail"</span>))</code></pre></div>
<pre><code> Error in force(code) : 
  sparklyr worker rscript failure, check worker logs for details</code></pre>
<p>In local mode, <code>sparklyr</code> will retrieve the logs for you. The logs point out the real failure as <code>ERROR sparklyr: RScript (4190) Make this fail</code> as you might expect.</p>
<pre><code>---- Output Log ----
(17/07/27 21:24:18 ERROR sparklyr: Worker (2427) is shutting down with exception ,java.net.SocketException: Socket closed)
17/07/27 21:24:18 WARN TaskSetManager: Lost task 0.0 in stage 389.0 (TID 429, localhost, executor driver): 17/07/27 21:27:21 INFO sparklyr: RScript (4190) retrieved 150 rows 
17/07/27 21:27:21 INFO sparklyr: RScript (4190) computing closure 
17/07/27 21:27:21 ERROR sparklyr: RScript (4190) Make this fail </code></pre>
<p>It is worth mentioning that different cluster providers and platforms expose worker logs in different ways. Specific documentation for your environment will point out how to retrieve these logs.</p>
</div>
<div id="requirements" class="section level2">
<h2 class="hasAnchor">
<a href="#requirements" class="anchor"></a>Requirements</h2>
<p>The <strong>R Runtime</strong> is expected to be pre-installed in the cluster for <code>spark_apply</code> to function. Failure to install the cluster will trigger a <code>Cannot run program, no such file or directory</code> error while attempting to use <code><a href="../reference/spark_apply.html">spark_apply()</a></code>. Contact your cluster administrator to consider making the R runtime available throughout the entire cluster.</p>
<p>A <strong>Homogenius Cluster</strong> is required since the driver node distributes, and potentially compiles, packages to the workers. For instance, the driver and workers must have the same processor architecture, system libraries, etc.</p>
</div>
<div id="configuration" class="section level2">
<h2 class="hasAnchor">
<a href="#configuration" class="anchor"></a>Configuration</h2>
<p>The following table describes relevant parameters while making use of <code>spark_apply</code>.</p>
<table class="table">
<colgroup>
<col width="38%">
<col width="61%">
</colgroup>
<thead><tr class="header">
<th>Value</th>
<th>Description</th>
</tr></thead>
<tbody>
<tr class="odd">
<td><code>spark.r.command</code></td>
<td>The path to the R binary. Useful to select from multiple R versions.</td>
</tr>
<tr class="even">
<td><code>sparklyr.worker.gateway.address</code></td>
<td>The gateway address to use under each worker node. Defaults to <code>sparklyr.gateway.address</code>.</td>
</tr>
<tr class="odd">
<td><code>sparklyr.worker.gateway.port</code></td>
<td>The gateway port to use under each worker node. Defaults to <code>sparklyr.gateway.port</code>.</td>
</tr>
</tbody>
</table>
<p>For example, one could make use of an specific R version by running:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">config &lt;-<span class="st"> </span><span class="kw"><a href="../reference/spark_config.html">spark_config</a></span>()
config[[<span class="st">"spark.r.command"</span>]] &lt;-<span class="st"> "&lt;path-to-r-version&gt;"</span>

sc &lt;-<span class="st"> </span><span class="kw"><a href="../reference/spark-connections.html">spark_connect</a></span>(<span class="dt">master =</span> <span class="st">"local"</span>, <span class="dt">config =</span> config)
<span class="kw"><a href="../reference/sdf_len.html">sdf_len</a></span>(sc, <span class="dv">10</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw"><a href="../reference/spark_apply.html">spark_apply</a></span>(<span class="cf">function</span>(e) e)</code></pre></div>
</div>
<div id="limitations" class="section level2">
<h2 class="hasAnchor">
<a href="#limitations" class="anchor"></a>Limitations</h2>
<div id="closures" class="section level3">
<h3 class="hasAnchor">
<a href="#closures" class="anchor"></a>Closures</h3>
<p>Closures are serialized using <code>serialize</code>, which is described as “A simple low-level interface for serializing to connections.”. One of the current limitations of <code>serialize</code> is that it wont serialize objects being referenced outside of it’s environment. For instance, the following function will error out since the closures references <code>external_value</code>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">external_value &lt;-<span class="st"> </span><span class="dv">1</span>
<span class="kw"><a href="../reference/spark_apply.html">spark_apply</a></span>(iris_tbl, <span class="cf">function</span>(e) e <span class="op">+</span><span class="st"> </span>external_value)</code></pre></div>
</div>
<div id="livy" class="section level3">
<h3 class="hasAnchor">
<a href="#livy" class="anchor"></a>Livy</h3>
<p>Currently, Livy connections do not support distributing packages since the client machine where the libraries are precompiled might not have the same processor architecture, not operating systems that the cluster machines.</p>
</div>
<div id="computing-over-groups" class="section level3">
<h3 class="hasAnchor">
<a href="#computing-over-groups" class="anchor"></a>Computing over Groups</h3>
<p>While performing computations over groups, <code><a href="../reference/spark_apply.html">spark_apply()</a></code> will provide partitions over the selected column; however, this implies that each partition can fit into a worker node, if this is not the case an exception will be thrown. To perform operations over groups that exceed the resources of a single node, one can consider partitioning to smaller units or use <code><a href="http://dplyr.tidyverse.org/reference/do.html">dplyr::do</a></code> which is currently optimized for large partitions.</p>
</div>
<div id="package-installation" class="section level3">
<h3 class="hasAnchor">
<a href="#package-installation" class="anchor"></a>Package Installation</h3>
<p>Since packages are copied only once for the duration of the <code><a href="../reference/spark-connections.html">spark_connect()</a></code> connection, installing additional packages is not supported while the connection is active. Therefore, if a new package needs to be installed, <code><a href="../reference/spark-connections.html">spark_disconnect()</a></code> the connection, modify packages and reconnect.</p>
</div>
</div>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="sidebar">
        <div id="tocnav">
      <h2 class="hasAnchor">
<a href="#tocnav" class="anchor"></a>Contents</h2>
      <ul class="nav nav-pills nav-stacked">
<li><a href="#overview">Overview</a></li>
      <li><a href="#distributing-packages">Distributing Packages</a></li>
      <li><a href="#handling-errors">Handling Errors</a></li>
      <li><a href="#requirements">Requirements</a></li>
      <li><a href="#configuration">Configuration</a></li>
      <li><a href="#limitations">Limitations</a></li>
      </ul>
</div>
      </div>

</div>


      <footer><div class="copyright">
  <p>Developed by Javier Luraschi, Kevin Ushey, JJ Allaire,  The Apache Software Foundation.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="http://hadley.github.io/pkgdown/">pkgdown</a>.</p>
</div>

      </footer>
</div>

  </body>
</html>
