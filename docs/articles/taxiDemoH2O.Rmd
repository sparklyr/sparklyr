---
title: "Data Science Toolchain with Spark and R"
subtitle: "Analyzing a billion NYC taxi trips in Spark"
output: 
  html_notebook:
    toc: true
    toc_float: true
---

# Overview

You can use Spark and R to analyze data at scale. This document describes how to use sparklyr to access and understand understand your data. Use the following tools in the toolchain:

<center><div style="width:450px">
![R for Data Science http://r4ds.had.co.nz/](http://r4ds.had.co.nz/diagrams/data-science.png)
</div></center>

* Import: Use `sdf_copy_to` to load a lookup table into spark
* Wrangle: Use `dplyr` and Spark SQL to manipulate data at scale
* Visualize: Use Spark SQL wrappers to aggregate data; then plot with `ggplot2`
* Model: Compare Spark ML, H2O Sparkling Water, and R models
* Communicate: Render the notebook in HTML

# Install extensions

Create your own R packages with interfaces to Spark. The `rsparkling` extension from H2O enables the use of additional models via sparklyr.

```{r setup, eval=FALSE}
install.packages("rsparkling")
install.packages("h2o", type = "source",
                 repos = "http://h2o-release.s3.amazonaws.com/h2o/rel-turnbull/2/R")
```

# Access

Create a spark context and load sparklyr extensions. Tune the cluster with configuration parameters.

```{r connect, message=FALSE, warning=FALSE}
# Load libraries
library(sparklyr)
library(tidyverse)
library(leaflet)
library(rsparkling)
library(h2o)
library(DT)

# Set environ vars
Sys.setenv(SPARK_HOME="/usr/lib/spark")

options(rsparkling.sparklingwater.version = '2.0.3')

# Configure cluster (c3.4xlarge 30G 16core 320disk)
conf <- spark_config()
conf$'sparklyr.shell.executor-memory' <- "20g"
conf$'sparklyr.shell.driver-memory' <- "20g"
conf$spark.executor.cores <- 16
conf$spark.executor.memory <- "20G"
conf$spark.yarn.am.cores  <- 16
conf$spark.yarn.am.memory <- "20G"
conf$spark.executor.instances <- 8
conf$spark.dynamicAllocation.enabled <- "false"
conf$maximizeResourceAllocation <- "true"
conf$spark.default.parallelism <- 32

# Connect to cluster
sc <- spark_connect(master = "yarn-client", config = conf, version = '2.0.0')

# Check H2O
h2o_context(sc)
```

# Understand

The NYC taxi data contain over 1 billion records describing pickups and drop offs. We will use `dplyr`, `ggplot`, and various modeling techniques to understand the NYC taxi data.

## Transform

It is rare that you get the data in exactly the right form you need. Often you will need to create some new variables or summaries, or maybe you just want to rename the variables or reorder the observations in order to make the data a little easier to work with.

```{r data}
# Load lookup table via R
nyct2010_tbl <- read_csv("~/sparkDemos/dev/h2o-demo/nyct2010.csv") %>%
  sdf_copy_to(sc, ., "nyct2010", overwrite = TRUE)

# Join tables
trips_par_tbl <- tbl(sc, "trips_par")
trips_joined_tbl <- trips_par_tbl %>%
  filter(!is.na(pickup_nyct2010_gid) & !is.na(dropoff_nyct2010_gid)) %>%
  filter(cab_type_id %in% c(1, 2)) %>%
  mutate(cab_type = ifelse(cab_type_id == 1, "yellow", "green")) %>%
  mutate(pay_type = ifelse(
    lower(payment_type) %in% c('2', 'csh', 'cash', 'cas'), "cash", ifelse(
      lower(payment_type) %in% c('1', 'crd', 'credit', 'cre'), "credit", "unk"))) %>%
  mutate(other_amount = round(total_amount - fare_amount - tip_amount, 2)) %>%
  left_join(
    select(nyct2010_tbl, pickup_gid = gid, 
           pickup_boro = boroname, pickup_nta = ntaname), 
    by = c("pickup_nyct2010_gid" = "pickup_gid")) %>%
  left_join(
    select(nyct2010_tbl, dropoff_gid = gid, 
           dropoff_boro = boroname, dropoff_nta = ntaname), 
    by = c("dropoff_nyct2010_gid" = "dropoff_gid")) %>%
    select(pickup_datetime, pickup_latitude, pickup_longitude, 
         pickup_nyct2010_gid, pickup_boro, pickup_nta,
         dropoff_datetime, dropoff_latitude, dropoff_longitude, 
         dropoff_nyct2010_gid, dropoff_boro, dropoff_nta,
         cab_type, passenger_count, trip_distance, 
         pay_type, fare_amount, tip_amount, other_amount, total_amount) %>%
  sdf_register("trips_par_joined")

# Save
#spark_write_parquet(trips_joined_tbl, "hdfs:///user/rstudio/trips_model_data")
```

Verify that the NYC taxi data has over 1 billion records.

```{r count}
# Calculate total trips
trips_model_data_tbl <- tbl(sc, "trips_model_data")
trips_model_data_tbl %>% count
```

## Visualize

R has several systems for making graphs, but ggplot2 is one of the most elegant and most versatile. ggplot2 implements the grammar of graphics, a coherent system for describing and building graphs. With ggplot2, you can do more faster by learning one system and applying it in many places.

Visualize all pickups in the NYC data. Notice most pickups occur at the airports and in Manhattan.

```{r pickups}
source("sqlvis_raster.R")
trips_model_data_tbl %>%
  sqlvis_compute_raster("pickup_longitude", "pickup_latitude") %>%
  sqlvis_ggplot_raster(title = "All Pickups")
```

Examine the distributions for fare amount and tip amount. Notice that most tips are under 25 and most trip fares are under 100 dollars. Limit the data to these cutoffs going forward.

```{r cutoff}
source("sqlvis_histogram.R")

trips_model_data_tbl %>%
  filter(fare_amount > 0 & fare_amount < 100) %>%
  filter(tip_amount > 0 & tip_amount < 25) %>%
  sqlvis_compute_histogram("tip_amount") %>%
  sqlvis_ggplot_histogram(title = "Tip amount")

trips_model_data_tbl %>%
  filter(fare_amount > 0 & fare_amount < 100) %>%
  filter(tip_amount > 0 & tip_amount < 25) %>%
  filter(pay_type == "credit") %>%
  sqlvis_compute_histogram("fare_amount") %>%
  sqlvis_ggplot_histogram(title = "Fare amount")
```

Plot the relationship between fares and tips. Notice that most people pay tips based on 10, 15, 20 percent increments. Overlay these percentages on the scatter plot.

```{r}
trips_model_data_tbl %>%
  filter(fare_amount > 0 & fare_amount < 100) %>%
  filter(tip_amount > 0 & tip_amount < 25) %>%
  filter(pickup_boro == "Manhattan" & dropoff_boro == "Brooklyn") %>%
  sqlvis_compute_raster("fare_amount", "tip_amount") %>%
  sqlvis_ggplot_raster(title = "Tip and Fare Correlation") -> p

p
p + geom_abline(intercept = 0, 
                slope = c(10,15,20,22,25,27,30,33)/25, 
                col = 'red', alpha = 0.2, size = 1)
```

Examine the relationship between tips and fares by payment types.

```{r facets}
trips_model_data_tbl %>%
  filter(fare_amount > 0 & fare_amount < 100) %>%
  filter(tip_amount > 0 & tip_amount < 25) %>%
  sqlvis_compute_raster_g("fare_amount", "tip_amount", "pay_type") %>%
  sqlvis_ggplot_raster_g(title = "Tip and Fare Correlation by Pay Type", ncol = 3)
```

Plot the most popular drop offs from JFK airport. Zoom into the map to see that one of the popular drop off locations is Wall Street.

```{r htmlwidgets}
# Summarize trips from JFK Airport
jfk_pickup_tbl <- trips_model_data_tbl %>%
  filter(pickup_nta == "Airport") %>%
  filter(!is.na(dropoff_nyct2010_gid)) %>%
  mutate(trip_time = unix_timestamp(dropoff_datetime) - unix_timestamp(pickup_datetime)) %>%
  group_by(dropoff_nyct2010_gid) %>% 
  summarize(n = n(),
            trip_time_mean = mean(trip_time),
            trip_dist_mean = mean(trip_distance),
            dropoff_latitude = mean(dropoff_latitude),
            dropoff_longitude = mean(dropoff_longitude),
            passenger_mean = mean(passenger_count),
            fare_amount = mean(fare_amount),
            tip_amount = mean(tip_amount))

# Collect top results
jfk_pickup <- jfk_pickup_tbl %>%
  mutate(n_rank = min_rank(desc(n))) %>%
  filter(n_rank <= 25) %>%
  collect

# Plot top trips on map
leaflet(jfk_pickup) %>% 
  setView(lng = -73.9, lat = 40.7, zoom = 11) %>%
  addProviderTiles("CartoDB.Positron") %>%
  addCircleMarkers(~dropoff_longitude, ~dropoff_latitude, stroke = F, color = "red") %>%
  addCircleMarkers(-73.7781, 40.6413, fill = FALSE, color = "green")
```

## Model

The goal of a model is to provide a simple low-dimensional summary of a dataset. Ideally, the model will capture true *signals* (i.e. patterns generated by the phenomenon of interest), and ignore *noise* (i.e. random variation that you are not interested in).

## Model Data

Create a model dataset for trips from Turtle Bay-East Midtown to JFK airport. Impose cutoffs cutoffs and break the data int test and training partitions. Cache the tables in order to improve performance of modeling techniques. 

```{r counts}
# Select a model data set
model_tbl <- trips_model_data_tbl %>%
  filter(fare_amount > 0 & fare_amount < 100) %>%
  filter(tip_amount >= 0 & tip_amount < 25) %>%
  filter(passenger_count > 0 & passenger_count < 5) %>%
  filter(pickup_nta == "Turtle Bay-East Midtown" & dropoff_nta == "Airport") %>%
  select(tip_amount, fare_amount, pay_type, passenger_count) 

# Partitioin into train and validate
model_partition_tbl <- model_tbl %>%
  sdf_partition(train = 0.1, test = 0.1, seed = 4321)

# Create table references
trips_train_tbl <- sdf_register(model_partition_tbl$train, "trips_train")
trips_test_tbl <- sdf_register(model_partition_tbl$test, "trips_test")

# Cache
tbl_cache(sc, "trips_train")
tbl_cache(sc, "trips_test")
```

## Train ML model

Build a simple linear regression using Spark ML. Use standard R formulas with Spark ML. Notice that people who pay with credit pay significantly more on average than people who pay in cash.

```{r ml_fit}
model_formula <- formula(tip_amount ~ fare_amount + pay_type + passenger_count)
m1 <- ml_linear_regression(trips_train_tbl, model_formula)
summary(m1)
```

### Test ML model

Score the model using `sdf_predict`. Plot the residuals to evaluate model assumptions.

```{r ml_pred}
# Score the predictions
pred_ml_tbl <- sdf_predict(m1, trips_test_tbl)

pred_ml_tbl %>%
  mutate(res = tip_amount - prediction) %>%
  sqlvis_compute_histogram("res") %>%
  sqlvis_ggplot_histogram(title = "Residuals")

pred_ml_tbl %>%
  mutate(residual = tip_amount - prediction) %>%
  sqlvis_compute_raster("prediction", "residual") %>%
  sqlvis_ggplot_raster(title = "Residuals vs Fitted")
```

## H2O Model

Use `rsparkling` to convert Spark DataFrames to H2O Frames. Use H2O to fit a linear model.

```{r h2o_glm}
# Convert to H2O Frames
trips_train_h2o_tbl <- as_h2o_frame(sc, trips_train_tbl)
trips_test_h2o_tbl <- as_h2o_frame(sc, trips_test_tbl)

# Format H2O Frames
trips_train_h2o_tbl$pay_type <- as.factor(trips_train_h2o_tbl$pay_type)
trips_test_h2o_tbl$pay_type <- as.factor(trips_test_h2o_tbl$pay_type)

# Fit model on H2O train data
m2 <- h2o.glm(
  x = c("fare_amount", "pay_type", "passenger_count"), 
  y = "tip_amount", 
  trips_train_h2o_tbl, 
  alpha = 0, 
  lambda = 0
  )
summary(m2)
```

## H2O deep learning

Fit a model using H2O's deep learning algorithm.

```{r h2o_dl}
m3 <- h2o.deeplearning(
  x = c("fare_amount", "pay_type", "passenger_count"), 
  y = "tip_amount", 
  training_frame = trips_train_h2o_tbl, 
  epochs = 2, 
  seed = 999
  )
summary(m3)
```

## Collect into R

Model in R. Notice the modeling dataset is only 1 million records, a small fraction of the overall data. It isn't unusual for model data to be orders of magnitude smaller than input data.

```{r nrow}
# Check number of rows
nrow(trips_train_tbl)
```

Collect the data into R and build a linear model and a regression tree using traditional R techniques.

```{r collect}
# Collect
trips_train <- collect(trips_train_tbl)
trips_test <- collect(trips_test_tbl)

# Format factors
trips_train$pay_type <- as.factor(trips_train$pay_type)
trips_test$pay_type <- as.factor(trips_test$pay_type)

# Fit linear model
m4 <- glm(model_formula, data = trips_train)
summary(m4)

# Regression tree
library(rpart)
m5 <- rpart(model_formula, trips_train)
summary(m5)
```

## Compare models

Compare the following models:

* Spark ML linear model
* H2O linear model
* R linear model
* H2O deep learning
* R regression tree

```{r compare}
# Score H2O test data
prediction_m2 <- h2o.predict(m2, newdata = trips_test_h2o_tbl)
prediction_m3 <- h2o.predict(m3, newdata = trips_test_h2o_tbl)
pred_h2o <- data.frame(
  tip_amount = trips_test$tip_amount,
  as_spark_dataframe(sc, prediction_m2),
  as_spark_dataframe(sc, prediction_m3)
)

# Score other models
trips_test$pred_lm <- predict(m4, trips_test)
trips_test$pred_rpart <- predict(m5, trips_test)

# Compute the mean squared error
mse <- function(formula, data) {
  data %>%
    mutate_(residual = formula) %>%
    summarize(mse = mean(residual ^ 2)) %>%
    collect %>%
    .[["mse"]]
}

# Summary
out <- data.frame(
  Train = c(
    ML_glm = m1$mean.squared.error,
    H2O_glm = h2o.mse(m2),
    H2O_deeplearning = h2o.mse(m3),
    R_glm = mean(m4$residuals^2),
    R_rpart = mean(resid(m5)^2)
    ),
  Test = c(
    ML_glm = mse(~ tip_amount - prediction, pred_ml_tbl),
    H2O_glm = mse(~ tip_amount - predict, pred_h2o),
    H2O_deeplearning = mse(~ tip_amount - predict.1, pred_h2o),
    R_glm = mse(~ tip_amount - pred_lm, trips_test),
    R_rpart = mse(~ tip_amount - pred_rpart, trips_test)
    )
  )
out
```


# Communicate

After controlling for fare amount, we found that trips paying with cash tip less on average than trips paying with credit. Additionally, trips with more passengers tend to tip less. 

The three linear models for Spark ML, H2O, and R all had equivalent fits but different output formats. The deep learning and regression tree models had the smallest mean square errors. The mean squared errors are smaller for the training data than the test data.

```{r summary}
out %>%
  mutate(Model = rownames(out)) %>%
  ggplot(aes(x = reorder(Model, Test), y = Test)) + 
  geom_bar(stat = "Identity") + 
  coord_flip() +
  ggtitle("MSE for rest data by model")
```

