---
title: "Read a CSV file into a Spark DataFrame"
aliases:
  - reference/sparklyr/latest/spark_read_csv
---

    <div class="doc-page">

    <div class="doc-page-index">
    <ul data-gumshoe>
    <li><a href="#arguments">Arguments</a></li>
    
    <li><a href="#details">Details</a></li>

    <li><a href="#see-also">See also</a></li>
        </ul>
    </div>

    <div class="doc-page-body">

    
    <p>Read a tabular data file into a Spark DataFrame.</p>
    

    <div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class='fu'>spark_read_csv</span>(<span class='no'>sc</span>, <span class='no'>name</span>, <span class='no'>path</span>, <span class='kw'>header</span> <span class='kw'>=</span> <span class='fl'>TRUE</span>, <span class='kw'>columns</span> <span class='kw'>=</span> <span class='kw'>NULL</span>,
  <span class='kw'>infer_schema</span> <span class='kw'>=</span> <span class='fl'>TRUE</span>, <span class='kw'>delimiter</span> <span class='kw'>=</span> <span class='st'>","</span>, <span class='kw'>quote</span> <span class='kw'>=</span> <span class='st'>"\""</span>, <span class='kw'>escape</span> <span class='kw'>=</span> <span class='st'>"\\"</span>,
  <span class='kw'>charset</span> <span class='kw'>=</span> <span class='st'>"UTF-8"</span>, <span class='kw'>null_value</span> <span class='kw'>=</span> <span class='kw'>NULL</span>, <span class='kw'>options</span> <span class='kw'>=</span> <span class='fu'>list</span>(),
  <span class='kw'>repartition</span> <span class='kw'>=</span> <span class='fl'>0</span>, <span class='kw'>memory</span> <span class='kw'>=</span> <span class='fl'>TRUE</span>, <span class='kw'>overwrite</span> <span class='kw'>=</span> <span class='fl'>TRUE</span>, <span class='no'>...</span>)</code></pre></div>
    
    <h2 id="arguments">Arguments</h2>
    <table class="ref-arguments">

    <colgroup>
      <col class="name" />
      <col class="desc" />
    </colgroup>

    <tr>
      <td>sc</td>
      <td><p>A <code>spark_connection</code>.</p></td>
    </tr>
    <tr>
      <td>name</td>
      <td><p>The name to assign to the newly generated table.</p></td>
    </tr>
    <tr>
      <td>path</td>
      <td><p>The path to the file. Needs to be accessible from the cluster.
Supports the <samp>"hdfs://"</samp>, <samp>"s3a://"</samp> and <samp>"file://"</samp> protocols.</p></td>
    </tr>
    <tr>
      <td>header</td>
      <td><p>Boolean; should the first row of data be used as a header?
Defaults to <code>TRUE</code>.</p></td>
    </tr>
    <tr>
      <td>columns</td>
      <td><p>A vector of column names or a named vector of column types.</p></td>
    </tr>
    <tr>
      <td>infer_schema</td>
      <td><p>Boolean; should column types be automatically inferred?
Requires one extra pass over the data. Defaults to <code>TRUE</code>.</p></td>
    </tr>
    <tr>
      <td>delimiter</td>
      <td><p>The character used to delimit each column. Defaults to <samp>','</samp>.</p></td>
    </tr>
    <tr>
      <td>quote</td>
      <td><p>The character used as a quote. Defaults to <samp>'"'</samp>.</p></td>
    </tr>
    <tr>
      <td>escape</td>
      <td><p>The character used to escape other characters. Defaults to <samp>'\'</samp>.</p></td>
    </tr>
    <tr>
      <td>charset</td>
      <td><p>The character set. Defaults to <samp>"UTF-8"</samp>.</p></td>
    </tr>
    <tr>
      <td>null_value</td>
      <td><p>The character to use for null, or missing, values. Defaults to <code>NULL</code>.</p></td>
    </tr>
    <tr>
      <td>options</td>
      <td><p>A list of strings with additional options.</p></td>
    </tr>
    <tr>
      <td>repartition</td>
      <td><p>The number of partitions used to distribute the
generated table. Use 0 (the default) to avoid partitioning.</p></td>
    </tr>
    <tr>
      <td>memory</td>
      <td><p>Boolean; should the data be loaded eagerly into memory? (That
is, should the table be cached?)</p></td>
    </tr>
    <tr>
      <td>overwrite</td>
      <td><p>Boolean; overwrite the table with the given name if it
already exists?</p></td>
    </tr>
    <tr>
      <td>...</td>
      <td><p>Optional arguments; currently unused.</p></td>
    </tr>
    </table>
    
    <h2 id="details">Details</h2>

    <p>You can read data from HDFS (<code>hdfs://</code>), S3 (<code>s3a://</code>),
  as well as the local file system (<code>file://</code>).</p>
<p>If you are reading from a secure S3 bucket be sure to set the following in your spark-defaults.conf
<code>spark.hadoop.fs.s3a.access.key</code>, <code>spark.hadoop.fs.s3a.secret.key</code> or any of the methods outlined in the aws-sdk
documentation <a href='http://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/credentials'>Working with AWS credentials</a>
In order to work with the newer <code>s3a://</code> protocol also set the values for <code>spark.hadoop.fs.s3a.impl</code> and <code>spark.hadoop.fs.s3a.endpoint </code>.
In addition, to support v4 of the S3 api be sure to pass the <code>-Dcom.amazonaws.services.s3.enableV4</code> driver options
for the config key <code>spark.driver.extraJavaOptions </code>
For instructions on how to configure <code>s3n://</code> check the hadoop documentation:
<a href='https://hadoop.apache.org/docs/stable/hadoop-aws/tools/hadoop-aws/index#Authentication_properties'>s3n authentication properties</a></p>
<p>When <code>header</code> is <code>FALSE</code>, the column names are generated with a
<code>V</code> prefix; e.g. <code>V1, V2, ...</code>.</p>
    
    <h2 id="see-also">See also</h2>

    <p>Other Spark serialization routines: <code><a href='spark_load_table'>spark_load_table</a></code>,
  <code><a href='spark_read_jdbc'>spark_read_jdbc</a></code>,
  <code><a href='spark_read_json'>spark_read_json</a></code>,
  <code><a href='spark_read_libsvm'>spark_read_libsvm</a></code>,
  <code><a href='spark_read_parquet'>spark_read_parquet</a></code>,
  <code><a href='spark_read_source'>spark_read_source</a></code>,
  <code><a href='spark_read_table'>spark_read_table</a></code>,
  <code><a href='spark_read_text'>spark_read_text</a></code>,
  <code><a href='spark_save_table'>spark_save_table</a></code>,
  <code><a href='spark_write_csv'>spark_write_csv</a></code>,
  <code><a href='spark_write_jdbc'>spark_write_jdbc</a></code>,
  <code><a href='spark_write_json'>spark_write_json</a></code>,
  <code><a href='spark_write_parquet'>spark_write_parquet</a></code>,
  <code><a href='spark_write_source'>spark_write_source</a></code>,
  <code><a href='spark_write_table'>spark_write_table</a></code>,
  <code><a href='spark_write_text'>spark_write_text</a></code></p>
    

    </div>

    </div>

