---
title: "Spark SQL: Count Unique Values"
output:
  github_document:
    fig_width: 9
    fig_height: 5
---

This article shows how you might use the `dplyr` interface to Spark SQL to compute the number of times a particular value shows up in a DataFrame column.

First, we read our data into Spark.

```{r setup, results='hide', warning=FALSE, message=FALSE}
library(sparkapi)
library(sparklyr)
library(dplyr)
library(ggplot2)

data(diamonds, package = "ggplot2")

sc <- spark_connect(master = "local", version = "1.6.1")
diamonds_tbl <- copy_to(sc, diamonds, "diamonds", overwrite = TRUE)

sdf <- sdf_copy_to(sc, diamonds)
sdf %>%
  sdf_sample(fraction = 0.001) %>%
  sdf_collect()
```

Now that we have `diamonds_tbl` available, we can use the `dplyr` interface to compute the number of times each entry in the `cut` column occurs.

```{r}
diamonds_tbl %>%
  group_by(cut) %>%
  summarize(total = n())
```
